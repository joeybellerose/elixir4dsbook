# Workbook

```elixir
Mix.install([
  {:explorer, "~> 0.3.0"},
  {:nx, "~> 0.3.0"},
  {:vega_lite, "~> 0.1.6"},
  {:kino_vega_lite, "~> 0.1.3"}
])
```

## Introduction

When it comes to data, people are often looking for answers.  No one collects data just for the fun of it.  Some want to understand what happened in the past while others want a peak in the future.  While there is no crystal ball, you can create educated guesses with models.

The problem is that most data is messy and can't be used in it's raw form to create high quality models.  This could due to errors in how the data was collected, collecting the wrong kind of data or collecting the data without enough detail to name a few.  Either way, in each scenario you will need to use your Data Transformation skills to manipulate, edit and clean the data to get it into "shape".  The better job you do here, the better you will be able to answer you customers questions about the past and future.

In the following 6 sections you'll be going over:

* Filtering -> Find subsets within your dataset by keeping or removing rows to focus on the question at hand.
* Arranging -> Organize your data by sorting one or more columns by asending or descending order.  This is great for dates and time related information.
* Selecting -> Remove the clutter by keeping relevant columns or remove extraneous columns.
* Renaming -> Clean up your column names so they are easy to remember and refer to later on.
* Mutating -> Enhace your dataset by editing existing columns or creating new ones.
* Summarising -> Summarise your low-level data to create and show your insights.

In each section you'll be using the `flights` dataset to learn how each step of the Data Trasnformation process works. The dataset can be found [here](https://vincentarelbundock.github.io/Rdatasets/csv/nycflights13/flights.csv) and it's documentation [here](https://vincentarelbundock.github.io/Rdatasets/doc/nycflights13/flights.html).

```elixir
alias Explorer.{DataFrame, Series}
alias VegaLite, as: Vl
```

```elixir
# Read in Data
file_path = "~/Documents/r4ds/data/flights.csv"

df =
  DataFrame.from_csv!(file_path)
  |> DataFrame.select([""], :drop)
```

Note: When you initially load the `flights` dataset, you will notice a blank ("") column name.  This is the index that was saved with the DataFrame.  Explorer (Polars) does not use [indices](https://pola-rs.github.io/polars-book/user-guide/coming_from_pandas.html) in its DataFrames.  Thus, you can `:drop` the column as you will not need it.

## Filtering

Specific questions need specific answers.  The problem is that it's so easy for unwated data to make it's way into your results without you realizing it.  This is where filtering comes in. Filtering is a transformation method to remove unnecesary or unwanted rows of data.  For example, you may want to only focus on a subset of the total data you have.  Or you may need to remove the `nil`s from your dataset for a model to run properly.

Filtering is what helps you to get laser focused on the task at hand.  Typically in response to a question that you are trying to answer.  It prevents unnecessary data from affecting your results later on when you start summarising with counts, sums, and averages.

Without filtering properly, you may produce irrelavant or wrong answers due to "data leaks".  Think of filtering as creating a firewall between the subset of data you need and the rest of the dataset.  Whatever you do to the subset has no affect on the rest of the data.  The reverse of that is also true.  Either way filtering is a commonly used tool in the Data Transformation toolbag.

<!-- livebook:{"break_markdown":true} -->

A good starting point would be to find all the flights from January 1st

```elixir
df
|> DataFrame.filter_with(
  &(Series.equal(&1["month"], 1)
    |> Series.and(Series.equal(&1["day"], 1)))
)
```

### Comparisons

<!-- livebook:{"break_markdown":true} -->

Comparison are what you will use to determine if two items are equivalent or not.  If so the comparison will return `true`, and if not, it will return `false`

Explorer handles equality for you with the `equal` function so you don't need to worry about using `=` vs `==` vs `===` like you normally would.

For now, check out the simple example below.

```elixir
:math.sqrt(2) |> :math.pow(2) == 2
```

Notice how the result is `false`.  ButThe square root of 2 raised to the 2nd power should equal 2.  Why is that?  Looking at the first two operations will give you a hint.

```elixir
:math.sqrt(2) |> :math.pow(2)
```

Although the answer should be 2, you can see that it isn't.  This is due to computers having finite memory and needing to cut off the number of decimals they can store.  Thus there is some error introduced due to rounding.

Notice how you have the same problem in the example below.

```elixir
1 / 49 * 49 == 1
```

Elixir does not have a `near` function, like R does, to handle whether two numbers are approximately correct.

One solution to this problem would be to use the `round` function like this.

```elixir
# Take the square root
:math.sqrt(2)
# Raise it to the power of 2
|> :math.pow(2)
# Round the number to the nearest whole number
|> Float.round() == 2
```

```elixir
(1 / 49 * 49) |> Float.round() == 1
```

Notice how using the `round` function, you were able to "undo" the effects of the rouding that the computer made when calculating the formula in the first place.

<!-- livebook:{"break_markdown":true} -->

### Logical Operators

<!-- livebook:{"break_markdown":true} -->

Logical operators can often be tricky and can get complex quickly when dealing with more than one.  In the book R fo Data Science, there is an excellent [venn diagram](https://r4ds.had.co.nz/transform.html#logical-operators) to help you remember how and(&), or(|) and not(!) work together to create the right combination of two datasets.

<!-- livebook:{"break_markdown":true} -->

![](https://d33wubrfki0l68.cloudfront.net/01f4b6d39d2be8269740a3ad7946faa79f7243cf/8369a/diagrams/transform-logical.png)

<!-- livebook:{"break_markdown":true} -->

To start off, find all the flights that departed in November or December.

```elixir
df
|> DataFrame.filter_with(
  &(Series.equal(&1["month"], 11)
    |> Series.or(Series.equal(&1["month"], 12)))
)
```

...but what if you wanted to also include October?

You can create a pipe within a pipe by combining the `equal` and `or` functions withing the `Series` module to solve this.

```elixir
df
|> DataFrame.filter_with(
  &(Series.equal(&1["month"], 11)
    |> Series.or(Series.equal(&1["month"], 12))
    |> Series.or(Series.equal(&1["month"], 10)))
)
```

Next, find the flights that weren't delayed (arrival or departure) by more than 2 hours.

> Note: As of this writing, you cannot use the `!` to create the opposite of the filter you are creating.  Simply use the opposite function like `less_equal` (<=) instead of `greater` (>).  Or you could use `greater_equal` (>=) instead of `less` (<).

> `||` and `&&` will not work in place of `Series.or` and `Series.and` for combining conditions.

```elixir
df
|> DataFrame.filter_with(
  &(Series.less_equal(&1["arr_delay"], 120)
    |> Series.or(Series.less_equal(&1["dep_delay"], 120)))
)
```

### Missing Values

Missing values occur in almost every dataset.  Thus, you need yo have a way to handle and work with them.  Below are a couple functions that you can use with and without DataFrames.

`#N/A` in R is equivalent to `nil` in Elixir.

```elixir
nil == nil
```

Check for `nil` with `is_nil`

```elixir
x = nil

is_nil(x)
```

`Filter` in Explorer does not automatically remove `nil`s from the dataset.  You must explicitally do so if you want them removed.

Note: You can drop all rows with `nil`s by using the `drop_nil` function.

```elixir
df
|> DataFrame.drop_nil()
```

Now, say you didn't want to remove every row with a `nil` from your DataFrame, but simply wanted to remove all the `nil`s from a specific column.  How would you go about that?  This is where the `is_nil` function comes in.  It will find all the `nil` rows in a column so you can either keep or remove them.

With that, find all the missing `air_time` data.

```elixir
df
|> DataFrame.filter_with(&Series.is_nil(&1["air_time"]))
```

Although you may have a feel for how filtering works in Explorer,you've just scratched the surface of what can be done with filtering from the examples above.  If you'd like more examples, you can dig into the [documentation](https://hexdocs.pm/explorer/Explorer.DataFrame.html#filter_with/2).

## Arrange

Arrange is how you can sort the rows of your dataset into an order you choose.  To accomplish this you'll be using the `arrange` function.

In it, you can pass each column you want to sort on along with which way you want to sort it (i.e. descending or ascending).  The datasets is sorted based on the order you provide the `arrange` function.

For example, sort the flights dataset based on `month` and `day` in descending order.

```elixir
df
|> DataFrame.arrange(desc: "month", desc: "day")
```

Notice how the dataframe was sorted by starting with December 31st and working it's way backwards.

All missing values (`nil`) will be sorted at the beginning of the column.  You can see the example below by sorting the `dep_time` column in ascending order.

```elixir
df
|> DataFrame.arrange(asc: "dep_time")
```

## Select

Select is to columns as Filter is to rows.  Select gives you the ability to choose a subset of columns as shown below.

Why do you want to select a subset of columns -> Sometimes there may be extraneous data that you simple don't need.  Or maybe you want to focus on a few columns during your analysis and don't want to see all the other columns.  This can be especially useful when looking at large datasets with lots of columns.

Selecing only the essential columns can help you focus on the task at hand and help refine the problem you are trying to solve.  This is an important process even if you end up keeping many of the other columns.  Getting laser focused on the problem at hand will save you time later on.

```elixir
df
|> DataFrame.select(["year", "month", "day"])
```

One useful option for the `select` function is the `keep_or_drop` parameter.  By default, this parameter is set to `:keep` but you can use `:drop` to remove the columns specified as shown in the example below.

```elixir
df
|> DataFrame.select(["year", "month", "day"], :drop)
```

As of the time of this writing, it is not possible to select multiple adjacent columns with the names of the first and last columns like R does `year:day`.  One work around is to specify the numbers for the first and last column that you want like shown below.

```elixir
df
|> DataFrame.select(0..2)
```

...and you can also `:drop` to remove adjacent columns as well.

```elixir
df
|> DataFrame.select(0..2, :drop)
```

You can use standard Elixir String functions for more flexible ways to `select` columns as shown in the examples below.

<!-- livebook:{"break_markdown":true} -->

`String.starts_with` searches all the column names to see which ones exactly begin with the letters you supply.  Below will find all the column names that start with the letter "d".

```elixir
df
|> DataFrame.select(&String.starts_with?(&1, "d"))
```

`String.ends_with` searches all the column names to see which ones exactly end with the letters you supply.  Below will find all the columns names that end with the letter "e".

```elixir
df
|> DataFrame.select(&String.ends_with?(&1, "e"))
```

`String.contains` searches all the column names to see which ones contain anywhere the letters you supply.  Below will find any column name that contains the letter "a".

```elixir
df
|> DataFrame.select(&String.contains?(&1, "a"))
```

`String.match` matches the column names based on the regular expression you provide.  Below, will show you all the columns that have `_arr` in their name.

```elixir
df
|> DataFrame.select(&String.match?(&1, ~r/_[arr]/))
```

As you can see there is a lot of flexibility that Elixir natively gives you for selecting columns.  But there's even more options that what you've seen above.  Simply check out the Elixir String [docs](https://hexdocs.pm/elixir/1.13/String.html) to see what else you can use.

## Rename

For renaming column names, you should use the `rename` or `rename_with` functions. `rename` is for renaming columns individually while `rename_with` is for renaming all of the columns or a subset of columns in the same way.

<!-- livebook:{"break_markdown":true} -->

For example, to capitalize the `year` and `month` columns you could do the following:

```elixir
df
|> DataFrame.rename(year: "YEAR", month: "MONTH")
```

If you wanted to capitalize all the columns you could use `rename_with`:

```elixir
df
|> DataFrame.rename_with(&String.upcase(&1))
```

To take it one step further, you could capitalize only the columns that start with the letter "d".

```elixir
df
|> DataFrame.rename_with(&String.starts_with?(&1, "d"), &String.upcase(&1))
```

## Mutate

What is mutate -> a way to add new columns or edit existing ones

Why mutate -> You don't always have all the information you need in your dataset and will often need to create extra variables (columns).

Why don't you have all the info? -> Often times you're trying to answer questions that haven't been answered before.  If they have, the information most likely will already exist, unless you're the first one to automate it.

Why answer unanswered questions -> The quest to seek more knowledge drives creation of new data and variables.

<!-- livebook:{"break_markdown":true} -->

You'll start this section be selecting a subset of the flights DataFrame. In R, you can do a complex `select` by combining multiple approaches in one function as shown below.

<!-- livebook:{"break_markdown":true} -->

```r
flights_sml <- select(flights, 
  year:day, 
  ends_with("delay"), 
  distance, 
  air_time
)
```

<!-- livebook:{"break_markdown":true} -->

Note: As of the writing of this material, Elixir/Explorer does not have a simple and concise way to select columns in multiple ways.  That doesn't mean it can't be done.  You just need to reach a little deeper in your Elixir toolbag.  You can achieve the same result by adding a pipe within a pipe as shown below.

```elixir
df_sml =
  df
  |> DataFrame.select(
    df.names
    |> Enum.take(3)
    |> Enum.concat(Enum.filter(df.names, &String.ends_with?(&1, "delay")))
    |> Enum.concat(["distance", "air_time"])
  )
```

With the new dataframe, you can now calculate the `gain` and `speed` for each flight by using `mutate_with`

```elixir
df_sml
|> DataFrame.mutate_with(
  &[
    gain: Series.subtract(&1["dep_delay"], &1["arr_delay"]),
    speed:
      Series.divide(&1["distance"], &1["air_time"])
      |> Series.multiply(60)
  ]
)
```

If you want to refer to coumns you created in `mutate` you need to add another `mutate` to your pipe as shown below.

```elixir
df_sml
|> DataFrame.mutate(
  gain: &Series.subtract(&1["dep_delay"], &1["arr_delay"]),
  speed:
    &(Series.divide(&1["distance"], &1["air_time"])
      |> Series.multiply(60))
)
|> DataFrame.mutate(gain_per_hour: &Series.divide(&1["gain"], &1["speed"]))
```

As of the writing of this material, Explorer does not have a to keep only the new columns you just created like R does with `transmute`.

<!-- livebook:{"break_markdown":true} -->

Elixir, unlike R, does not have vectorised operators that work on columns of data.  For example, you could not simply subtract to columns in a DataFrame.

```elixir
df["dep_delay"] - df["arr_delay"]
```

* Arithmetic:

The basic elixir operators only work on individual numbers.  When you want to do math operations to entire columns of data you need to use Explorer's built in functions.  Below are a list of useful functions you will probably need when working with DataFrames.

<!-- livebook:{"force_markdown":true} -->

```elixir
Series.add()
Series.subtract()
Series.multiply()
Series.divide()
Series.pow()
```

<!-- livebook:{"break_markdown":true} -->

* Modular Arithmetic

Explorer does not have modular arithmetic built in at this time.  On the other hand, Elixir does have access to modular arithmetic through the erlang `:math` module.  Unfortumately, it does not work on individual Explorer Series (columns) as you can see below.

```elixir
df["dep_time"] |> :math.fmod(100)
```

* Logs

Explorer does not currently implement any logarithmic functions that will work on Series.  Polars does already have this [capability](https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.Series.log.html) which means it only needs to be brought into Explorer.

Note: You can use Erlang's `:math` module to calculate logarithms of individual numbers.

<!-- livebook:{"break_markdown":true} -->

* Offsets

Explorer does not currently implement any offset functions that will work on Series.  Polars does already have this [capability](https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.Series.shift.html) which means it only needs to be brought into Explorer.

<!-- livebook:{"break_markdown":true} -->

* Cumulative and Rolling Averages

Explorer provides you access to the following Cumulative functions

<!-- livebook:{"force_markdown":true} -->

```elixir
Series.cumulative_max
Series.cumulative_min
Series.cumulative_sum
```

...and the following Windows functions

<!-- livebook:{"force_markdown":true} -->

```elixir
Series.windows_max
Series.windows_mean
Series.windows_min
Series.windows_sum
```

...which you can see in the examples below.

<!-- livebook:{"break_markdown":true} -->

Cumulative Sum Example

```elixir
df["arr_time"]
|> Series.cumulative_sum()
```

Windows Sum Example

```elixir
df["arr_time"]
|> Series.window_sum(4)
```

> Note: Keep in mind how quick you just computed a cumulative and a rolling (window) sum.  These are typically expense operations.

<!-- livebook:{"break_markdown":true} -->

* Ranking

Explorer does not currently implement any ranking functions that will work on Series.  Polars does already have this [capability](https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.Series.rank.html) which means it only needs to be brought into Explorer.

```elixir
[1, 2, 3]
|> Enum.map(fn x -> :math.sqrt(x) end)
```

## Summarise

What does summarise do? -> Summarise creates summary information.

Why do we need summaries -> Need higher level information to present findings to people looking for answers like customers and/or leadership.

Why do others need summary information -> Not everyone has the time to go dig into the details of each dataset needed to answer their questions.  They rely on people like you to provide the information they can act on to make better decisions.

Why do they want better decisions -> Most people hate making mistakes due to the fear of failure.

<!-- livebook:{"break_markdown":true} -->

Find the average (`mean`) departure delay (`dep_delay`) from the flights dataset.

```elixir
df["dep_delay"]
|> Series.mean()
```

This task can be solved by simpling taking the `mean` of the enitre column.  Where summarizing data gets powerful is when you combine `summarise`/`summarise_with` with `group_by`.  Check out the example below.

<!-- livebook:{"break_markdown":true} -->

> Note: Explorer provides you both `summarise` and `summarise_with` to aggregate information.  Both ungroup after the transaction while the later has better performance and allows you to do more complex operations.  You can read more about them in the [docs](https://hexdocs.pm/explorer/Explorer.DataFrame.html#summarise_with/2).

<!-- livebook:{"break_markdown":true} -->

Find the average (`mean`) departure delay (`dep_delay`) for each `year`, `month` and `day` from the flights dataset.

```elixir
df
|> DataFrame.group_by(["year", "month", "day"])
|> DataFrame.summarise_with(&[dep_delay: Series.mean(&1["dep_delay"])])
```

One thing that may look wierd to you is the use of the `&`.  This is the "Capture Operator".  In Elixir, this is a shorthand way of writing an anonymous function. For example, instead of writing this

<!-- livebook:{"force_markdown":true} -->

```elixir
df
|> DataFrame.group_by(["year", "month", "day"])
|> DataFrame.summarise_with(
  fn x -> 
    [
      dep_delay: Series.mean(x["dep_delay"])
    ]
  end
)
```

...you can write this.

<!-- livebook:{"force_markdown":true} -->

```elixir
df
|> DataFrame.group_by(["year", "month", "day"])
|> DataFrame.summarise_with(
  &[
    dep_delay: Series.mean(&1["dep_delay"])
  ]
)
```

Either way, you need to use anonymous functions to specify which column you want to transform or summarize.

> Note: `summarise_with` expects for all you summary variables to be captured within the list shown above.

<!-- livebook:{"break_markdown":true} -->

You can read more about capture operator (&) and anonymous functions in the [docs](https://hexdocs.pm/elixir/Kernel.SpecialForms.html#&/1)

<!-- livebook:{"break_markdown":true} -->

### Combining multiple operations with the pipe

<!-- livebook:{"break_markdown":true} -->

If you haven't noticed already, most things in Elixir are done using the pipe (`|>`) operator.  With it, you can string transformations together into a pipeline that is both more readable and more consice than what you may have seen in other languages.

<!-- livebook:{"break_markdown":true} -->

For example, explore the relationship between the average arrival delay (`arr_delay`) and `distance` for each airport (`dest`).

Then remove all rows where there are less than 20 entries and the airport (`dest`) is not "HNL".

```elixir
delay =
  df
  |> DataFrame.group_by(["dest"])
  |> DataFrame.summarise_with(
    &[
      count: Series.count(&1["dest"]),
      dist: Series.mean(&1["distance"]),
      delay: Series.mean(&1["arr_delay"])
    ]
  )
  |> DataFrame.filter_with(&Series.greater(&1["count"], 20))
  |> DataFrame.filter_with(&Series.not_equal(&1["dest"], "HNL"))
```

In the code above there are 3 main steps.

1. Group all data by the airport.  This way, all future summaries will collapse down to one result per airpport.

2. Summarise the averages for the distance and delay  and count the number of flights for each airport.

3. Filter out noisy datapoints and remove Honolulu airport becase its so much further away than every other airport.

<!-- livebook:{"break_markdown":true} -->

You can now plot the `delay` DataFrame to see the realtionship between arrival delay and distance.

```elixir
Vl.new()
|> Vl.data_from_values(delay)
|> Vl.layers([
  Vl.new()
  |> Vl.mark(:circle)
  |> Vl.encode_field(:x, "dist", type: :quantitative)
  |> Vl.encode_field(:y, "delay", type: :quantitative)
  |> Vl.encode_field(:size, "count", type: :quantitative)
  |> Vl.encode(:opacity, value: 1 / 3),
  Vl.new()
  |> Vl.transform(loess: "delay", on: "dist", bandwidth: 0.65)
  |> Vl.mark(:line)
  |> Vl.encode_field(:x, "dist", type: :quantitative)
  |> Vl.encode_field(:y, "delay", type: :quantitative)
  |> Vl.encode(:color, value: "firebrick")
])
```

> Note: As mentioned in a previous chapter, the `layers` function will overlay two differnt charts.  In addition, the Loess regression option in the `transform` function is what provides the nice smooth curve overlaying the circles.

<!-- livebook:{"break_markdown":true} -->

You can now see that delays tend to reduce as the flight distance is longer.  Between ~500-750 miles have the greatest delays.

<!-- livebook:{"break_markdown":true} -->

### Missing values

<!-- livebook:{"break_markdown":true} -->

One benefit you get for free, is not needing to account for how to handle `nil`s when your summarizing information.  Explorer automatically handles them correctly for you so you won't have errors from including `nil`s in your operations.

<!-- livebook:{"break_markdown":true} -->

Explorer gives you a few different ways to deal with missing values.  It depends on whether you want to work with the whole DataFrame (table) or a Series (column).

**DataFrame**

* `drop_nil` -> Removes any row with a `nil` in any column.

**Series**

* `is_nil` -> Returns true for all the rows with a `nil` in a specific column
* `is_not_nil` -> Returns true for all the rows without a `nil` in a specific column

<!-- livebook:{"break_markdown":true} -->

Given the information above, remove all the cancelled flights to use in future sections.  Then, find the average departure delay (`dep_delay`) by `year`, `month` and `day`.

```elixir
not_cancelled =
  df
  # Filter 
  |> DataFrame.filter_with(&Series.is_not_nil(&1["dep_delay"]))
  |> DataFrame.filter_with(&Series.is_not_nil(&1["arr_delay"]))

not_cancelled
|> DataFrame.group_by(["year", "month", "day"])
|> DataFrame.summarise_with(
  &[
    mean: Series.mean(&1["dep_delay"])
  ]
)
```

### Counts

<!-- livebook:{"break_markdown":true} -->

`Series.count` is a commonly used way to aggregate data.

Why is `count` commonly used? ->

1. It helps you keep context over the data size you just aggregated.
2. It is often necessary for creating ratios with your aggregated data
3. It is important for deriving many statitstical results.  The `count` may end up being your sample or population size.

<!-- livebook:{"break_markdown":true} -->

For example, find the planes (`tailnum`) that have the highest average delays (`arr_delay`).

```elixir
delays =
  not_cancelled
  |> DataFrame.group_by(["tailnum"])
  |> DataFrame.summarise_with(
    &[
      delay: Series.mean(&1["dep_delay"])
    ]
  )
```

Now you can plot the results to see what's happening.

```elixir
Vl.new()
|> Vl.data_from_values(delays)
|> Vl.mark(:line)
|> Vl.encode_field(:x, "delay", bin: %{maxbins: 20}, type: :quantitative)
|> Vl.encode(:y, aggregate: :count, type: :quantitative)
```

By binning your `:x` variable, you can see that there a few planes with really long delays.  Before you draw any conclusions, add the number of flights (`count`) and compare that with the average delay (`delay`) with a scatterplot.

```elixir
delays =
  not_cancelled
  |> DataFrame.group_by(["tailnum"])
  |> DataFrame.summarise_with(
    &[
      delay: Series.mean(&1["dep_delay"]),
      count: Series.count(&1["tailnum"])
    ]
  )

Vl.new()
|> Vl.data_from_values(delays)
|> Vl.mark(:circle)
|> Vl.encode_field(:x, "count", type: :quantitative)
|> Vl.encode_field(:y, "delay", type: :quantitative)
|> Vl.encode(:opacity, value: 1 / 10)
|> Vl.encode(:color, value: "red")
```

As you can see from the chart above, all of the delays greater than 100 minutes come from groups with the smallent number of flights.  See what happens when you to remove the groups with less than 25 flights.

```elixir
delays =
  not_cancelled
  |> DataFrame.group_by(["tailnum"])
  |> DataFrame.summarise_with(
    &[
      delay: Series.mean(&1["dep_delay"]),
      count: Series.count(&1["tailnum"])
    ]
  )
  |> DataFrame.filter_with(&Series.greater(&1["count"], 25))

Vl.new(width: 400)
|> Vl.data_from_values(delays)
|> Vl.mark(:circle)
|> Vl.encode_field(:x, "count", type: :quantitative)
|> Vl.encode_field(:y, "delay", type: :quantitative)
|> Vl.encode(:opacity, value: 1 / 10)
|> Vl.encode(:color, value: "red")
```

Removing the smaller groups shows that the average departure delay for all aircraft is less than 1 hour.  This example highlights one reason why `:counts` matter and why they are so often used.

<!-- livebook:{"break_markdown":true} -->

### Useful summary functions

<!-- livebook:{"break_markdown":true} -->

* Measures of Location

In addition to `mean`, which you've aready seen, Explorer provides `median` for finding the mid-point in your Series.

```elixir
not_cancelled
|> DataFrame.group_by(["year", "month", "day"])
|> DataFrame.summarise_with(
  &[
    mean_delay: Series.mean(&1["arr_delay"]),
    median_delay: Series.median(&1["arr_delay"])
  ]
)
```

* Measures of Spread

Explorer comes out of the box with standard deviation (`sd`), but lacks some of the other statistical measures that R has; like Interquartile Range and Mean Absolute deviation

```elixir
not_cancelled
|> DataFrame.group_by(["dest"])
|> DataFrame.summarise_with(
  &[
    distance_sd: Series.std(&1["distance"])
  ]
)
|> DataFrame.arrange(desc: "distance_sd")
```

* Measures of Rank

`mix`, `max` and `quantile` all come with Explorer.  You can check out [this article](https://www.codecademy.com/learn/dscp-summary-statistics/modules/dscp-quartiles-quantiles-and-interquartile-range/cheatsheet), from Codeacademy, if you want a brief refresher of quantiles.

```elixir
not_cancelled
|> DataFrame.group_by(["year", "month", "day"])
|> DataFrame.summarise_with(
  &[
    first: Series.min(&1["dep_time"]),
    last: Series.max(&1["dep_time"])
  ]
)
```

* Measures of Position

Explorer makes getting the `first` and `last` elements straghtforward as shown below.

```elixir
not_cancelled
|> DataFrame.group_by(["year", "month", "day"])
|> DataFrame.summarise_with(
  &[
    first_dep: Series.first(&1["dep_time"]),
    last_dep: Series.last(&1["dep_time"])
  ]
)
```

* Counts

While you've already used the `count` function, Explorer also provides `n_distinct` to get the count of the unique values in a Series.

```elixir
not_cancelled
|> DataFrame.group_by(["dest"])
|> DataFrame.summarise_with(
  &[
    carriers: Series.n_distinct(&1["carrier"])
  ]
)
|> DataFrame.arrange(desc: "carriers")
```

> Note: Explorer provides the count right in the header of the DataFrame output.

<!-- livebook:{"break_markdown":true} -->

* Counts and Proportions of Logical Values

Explorer does not currently have the ability to filter within a summary, but you could add the `filter_with` function before `summarise_with`.

```elixir
not_cancelled
|> DataFrame.group_by(["year", "month", "day"])
|> DataFrame.filter_with(&Series.less(&1["dep_time"], 500))
|> DataFrame.summarise_with(
  &[
    n_early: Series.count(&1["dep_time"])
  ]
)
```

### Grouping by multiple variables

X -> Remove section

<!-- livebook:{"break_markdown":true} -->

### Ungrouping

X -> Remove Section

## Grouped mutates (and filters)
